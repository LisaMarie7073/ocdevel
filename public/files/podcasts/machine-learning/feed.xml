<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Machine Learning Guide</title>
        <link>http://ocdevel.com/podcasts/machine-learning</link>
        <image>
            <url>http://ocdevel.com/files/podcasts/machine-learning/art.jpg</url>
            <title>Machine Learning Guide</title>
            <link>http://ocdevel.com/podcasts/machine-learning</link>
        </image>
        <description>This series aims to teach you the high level fundamentals of machine learning from A to Z. I'll teach you the basic intuition, algorithms, and math. We'll discuss languages and frameworks, deep learning, and more. Audio may be an inferior medium to task; but with all our exercise, commute, and chores hours of the day, not having an audio supplementary education would be a missed opportunity. And where your other resources will provide you the machine learning trees, I’ll provide the forest. Additionally, consider me your syllabus. At the end of every episode I’ll provide the best-of-the-best resources curated from around the web for you to learn each episode’s details.</description>
        <language>en-us</language>
        <copyright>OCDevel copyright 2017</copyright>
        <atom:link href="http://ocdevel.com/files/podcasts/machine-learning/feed.xml" rel="self" type="application/rss+xml"/>
        <lastBuildDate>Sun, 04 Feb 2018 00:00:00 EST</lastBuildDate>
        <itunes:author>OCDevel</itunes:author>
        <itunes:summary>This series aims to teach you the high level fundamentals of machine learning from A to Z. I'll teach you the basic intuition, algorithms, and math. We'll discuss languages and frameworks, deep learning, and more. Audio may be an inferior medium to task; but with all our exercise, commute, and chores hours of the day, not having an audio supplementary education would be a missed opportunity. And where your other resources will provide you the machine learning trees, I’ll provide the forest. Additionally, consider me your syllabus. At the end of every episode I’ll provide the best-of-the-best resources curated from around the web for you to learn each episode’s details.</itunes:summary>
        <itunes:subtitle>Introduction and intuition on machine learning principles, algorithms, and math. Your 'start here' ML resource.</itunes:subtitle>
        <itunes:owner>
            <itunes:name>Tyler Renelle</itunes:name>
            <itunes:email>tylerrenelle@gmail.com</itunes:email>
        </itunes:owner>
        <itunes:explicit>No</itunes:explicit>
        <itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
        <itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
        <itunes:category text="Technology">
          <itunes:category text="Software How-To"/>
        </itunes:category>
        <pubDate>Wed, 01 Feb 2017 00:00:00 EST</pubDate>
        <item>
<title>28. Hyperparameters 2</title>
<link>undefined</link>
<pubDate>Sun, 04 Feb 2018 00:00:00 EST</pubDate>
<description>Hyperparameters part 2: hyper-search, regularization, SGD optimizers, scaling

## Episode

- Hyper optimization
** GridSearch, RandomSearch
** Bayesian Optimization (https://thuijskens.github.io/2016/12/29/bayesian-optimisation/)
- Regularization: Dropout, L2, L1
** DNNs = Dropout
** L2 = most common
** L1 = sparsity (zeros) &amp; feature-selection (rarer circumstances)
- Optimizers (SGD): Momentum -&gt; Adagrad -&gt; RMSProp -&gt; Adam -&gt; Nadam
** http://sebastianruder.com/optimizing-gradient-descent/index.html#visualizationofalgorithms
- Initializers: Zeros, Random Uniform, Xavier
- Scaling
** Feature-scaling: MinMaxScaler, StandardScaler, RobustScaler
** Features + inter-layer: Batch Normalization</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>8671d415236e9a9394a0c4aaa383e1ba</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>Hyperparameters part 2: hyper-search, regularization, SGD optimizers, scaling</itunes:subtitle>
<itunes:summary>Hyperparameters part 2: hyper-search, regularization, SGD optimizers, scaling

## Episode

- Hyper optimization
** GridSearch, RandomSearch
** Bayesian Optimization (https://thuijskens.github.io/2016/12/29/bayesian-optimisation/)
- Regularization: Dropout, L2, L1
** DNNs = Dropout
** L2 = most common
** L1 = sparsity (zeros) &amp; feature-selection (rarer circumstances)
- Optimizers (SGD): Momentum -&gt; Adagrad -&gt; RMSProp -&gt; Adam -&gt; Nadam
** http://sebastianruder.com/optimizing-gradient-descent/index.html#visualizationofalgorithms
- Initializers: Zeros, Random Uniform, Xavier
- Scaling
** Feature-scaling: MinMaxScaler, StandardScaler, RobustScaler
** Features + inter-layer: Batch Normalization</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>27. Hyperparameters 1</title>
<link>undefined</link>
<pubDate>Sat, 27 Jan 2018 00:00:00 EST</pubDate>
<description>Hyperparameters part 1: network architecture

## Episode

- Hypers future &amp; meta-learning
** We're always removing hypers. DL removed feature-engineering `
- Model selection
** Unsupervised? K-means Clustering =&gt; DL
** Linear? Linear regression, logistic regression
** Simple? Naive Bayes, Decision Tree (Random Forest, Gradient Boosting)
** Little data? Boosting
** Lots of data, complex situation? Deep learning
- Network
** Layer arch
  ** Vision? CNN
  ** Time? LSTM
  ** Other? MLP
  ** Trading LSTM =&gt; CNN decision
** Layer size design (funnel, etc)
  ** Face pics
  ** From BTC episode
  ** Don't know? Layers=1, Neurons=mean(inputs, output)
      https://stats.stackexchange.com/a/1097/107199
- Activations / nonlinearity
  https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6
** Output
  ** Sigmoid = predict probability of output, usually at output
  ** Softmax = multi-class
  ** Nothing = regression
** Relu family (Leaky Relu, Elu, Selu, ...) = vanishing gradient (gradient is constant), performance, usually better
** Tanh = classification between two classes, mean 0 important</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>f5a903d68c1ed04bd37a31175d456fc0</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>Hyperparameters part 1: network architecture</itunes:subtitle>
<itunes:summary>Hyperparameters part 1: network architecture

## Episode

- Hypers future &amp; meta-learning
** We're always removing hypers. DL removed feature-engineering `
- Model selection
** Unsupervised? K-means Clustering =&gt; DL
** Linear? Linear regression, logistic regression
** Simple? Naive Bayes, Decision Tree (Random Forest, Gradient Boosting)
** Little data? Boosting
** Lots of data, complex situation? Deep learning
- Network
** Layer arch
  ** Vision? CNN
  ** Time? LSTM
  ** Other? MLP
  ** Trading LSTM =&gt; CNN decision
** Layer size design (funnel, etc)
  ** Face pics
  ** From BTC episode
  ** Don't know? Layers=1, Neurons=mean(inputs, output)
      https://stats.stackexchange.com/a/1097/107199
- Activations / nonlinearity
  https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6
** Output
  ** Sigmoid = predict probability of output, usually at output
  ** Softmax = multi-class
  ** Nothing = regression
** Relu family (Leaky Relu, Elu, Selu, ...) = vanishing gradient (gradient is constant), performance, usually better
** Tanh = classification between two classes, mean 0 important</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>26. Project Bitcoin Trader</title>
<link>undefined</link>
<pubDate>Fri, 26 Jan 2018 00:00:00 EST</pubDate>
<description>Community project &amp; intro to Bitcoin/crypto + trading

## Resources
- TForce BTC Trader (https://github.com/lefnire/tforce_btc_trader) (podcast project)

## Episode

- Project: Trading Crypto
** Reasons
  ** Get rich
  ** Hot topic
  ** Special: Intuitively highlights decisions: hypers, supervised v reinforcement, LSTM v CNN
** Pros: skip this
- Crypto (v stock)
** Bitcoin, Ethereum, Litecoin, Ripple
** Many benefits (immutable permenant distributed ledger; security; low fees; international; etc)
** For our purposes: popular, volatile, singular
  ** Singular like Forex vs Stock (instruments)
- Trading basics
** Day, swing, investing
** Patterns (technical analysis, vs fundamentals)
** OHLCV / Candles
** Indicators
** Exchanges &amp; Arbitrage (GDAX, Krakken)
- Good because highlights lots
** LSTM v CNN
** Supervised v Reinforcement
** Obvious net architectures (indicators, time-series, tanh v relu)</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>e704eb47d4280a7abc9bb6f0895a7b26</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>Community project &amp; intro to Bitcoin/crypto + trading</itunes:subtitle>
<itunes:summary>Community project &amp; intro to Bitcoin/crypto + trading

## Resources
- TForce BTC Trader (https://github.com/lefnire/tforce_btc_trader) (podcast project)

## Episode

- Project: Trading Crypto
** Reasons
  ** Get rich
  ** Hot topic
  ** Special: Intuitively highlights decisions: hypers, supervised v reinforcement, LSTM v CNN
** Pros: skip this
- Crypto (v stock)
** Bitcoin, Ethereum, Litecoin, Ripple
** Many benefits (immutable permenant distributed ledger; security; low fees; international; etc)
** For our purposes: popular, volatile, singular
  ** Singular like Forex vs Stock (instruments)
- Trading basics
** Day, swing, investing
** Patterns (technical analysis, vs fundamentals)
** OHLCV / Candles
** Indicators
** Exchanges &amp; Arbitrage (GDAX, Krakken)
- Good because highlights lots
** LSTM v CNN
** Supervised v Reinforcement
** Obvious net architectures (indicators, time-series, tanh v relu)</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>25. Convolutional Neural Networks</title>
<link>undefined</link>
<pubDate>Mon, 30 Oct 2017 00:00:00 EST</pubDate>
<description>Convnets or CNNs. Filters, feature maps, window/stride/padding, max-pooling.

## Resources
- Stanford cs231n: Convnets (https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC) `course:medium`
- Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium`
- The usual DL resources (pick one):
** Deep Learning Book (http://amzn.to/2tXgCiT) (Free HTML version (http://www.deeplearningbook.org/)) `book:hard` comprehensive DL bible; highly mathematical
** Fast.ai (http://course.fast.ai/) `course:medium` practical DL for coders
  
## Episode

- One-time donations w/ BTC / PayPal
- Image recognition, classification - computer vision
** ML takeover
** Final main network (MLP, RNN, CNN)
- Don't use MLP for images, use CNNs
- Filters -&gt; feature maps -&gt; convolutional layers
- Window, stride, padding
- Max-pooling
- Architectures (ILSVRC ImageNet Challenge)
** LeNet-5
** AlexNet
** GoogLeNet
** Inception
** Resnet
** etc..</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>91bf8a0266bc22088c897eb756cc97d3</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>Convnets or CNNs. Filters, feature maps, window/stride/padding, max-pooling.</itunes:subtitle>
<itunes:summary>Convnets or CNNs. Filters, feature maps, window/stride/padding, max-pooling.

## Resources
- Stanford cs231n: Convnets (https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC) `course:medium`
- Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium`
- The usual DL resources (pick one):
** Deep Learning Book (http://amzn.to/2tXgCiT) (Free HTML version (http://www.deeplearningbook.org/)) `book:hard` comprehensive DL bible; highly mathematical
** Fast.ai (http://course.fast.ai/) `course:medium` practical DL for coders
  
## Episode

- One-time donations w/ BTC / PayPal
- Image recognition, classification - computer vision
** ML takeover
** Final main network (MLP, RNN, CNN)
- Don't use MLP for images, use CNNs
- Filters -&gt; feature maps -&gt; convolutional layers
- Window, stride, padding
- Max-pooling
- Architectures (ILSVRC ImageNet Challenge)
** LeNet-5
** AlexNet
** GoogLeNet
** Inception
** Resnet
** etc..</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>24. Tech Stack</title>
<link>undefined</link>
<pubDate>Fri, 06 Oct 2017 00:00:00 EST</pubDate>
<description>TensorFlow, Pandas, Numpy, Scikit-Learn, Keras, TensorForce.

## Resources
- Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium`
- The usual DL resources (pick one):
** Deep Learning Book (http://amzn.to/2tXgCiT) (Free HTML version (http://www.deeplearningbook.org/)) `book:hard` comprehensive DL bible; highly mathematical
** Fast.ai (http://course.fast.ai/) `course:medium` practical DL for coders
  
## Custom PC Build

Temporarily removed since the Titan V (https://www.nvidia.com/en-us/titan/titan-v/) was released, which succeeds my prior 1080ti build recommend. Keep an eye on https://pcpartpicker.com/builds/ for builds with that card (currently none).

## Episode

- Looking for work (https://www.linkedin.com/in/lefnire/)
- Autodiff frameworks
** TensorFlow
** PyTorch (Theano dead!)
** GPU: Nvidia + Ubuntu
** AWS v GCP v Azure (see AWS spot instances)
- Pandas, Numpy, ScikitLearn, TensorFlow
- Celery (RabbitMQ)
- Higher
** Keras
** TensorForce</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>11e604992dcd4f124cb4d3897c81056f</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>TensorFlow, Pandas, Numpy, Scikit-Learn, Keras, TensorForce.</itunes:subtitle>
<itunes:summary>TensorFlow, Pandas, Numpy, Scikit-Learn, Keras, TensorForce.

## Resources
- Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium`
- The usual DL resources (pick one):
** Deep Learning Book (http://amzn.to/2tXgCiT) (Free HTML version (http://www.deeplearningbook.org/)) `book:hard` comprehensive DL bible; highly mathematical
** Fast.ai (http://course.fast.ai/) `course:medium` practical DL for coders
  
## Custom PC Build

Temporarily removed since the Titan V (https://www.nvidia.com/en-us/titan/titan-v/) was released, which succeeds my prior 1080ti build recommend. Keep an eye on https://pcpartpicker.com/builds/ for builds with that card (currently none).

## Episode

- Looking for work (https://www.linkedin.com/in/lefnire/)
- Autodiff frameworks
** TensorFlow
** PyTorch (Theano dead!)
** GPU: Nvidia + Ubuntu
** AWS v GCP v Azure (see AWS spot instances)
- Pandas, Numpy, ScikitLearn, TensorFlow
- Celery (RabbitMQ)
- Higher
** Keras
** TensorForce</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>23. Deep NLP 2</title>
<link>undefined</link>
<pubDate>Sun, 20 Aug 2017 00:00:00 EST</pubDate>
<description>RNN review, bi-directional RNNs, LSTM &amp; GRU cells.

## Resources
- Overview Articles: 
** Unreasonable Effectiveness of RNNs (http://karpathy.github.io/2015/05/21/rnn-effectiveness/) `article:easy`
** Deep Learning, NLP, and Representations (http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/) `article:medium`
** Understanding LSTM Networks (http://colah.github.io/posts/2015-08-Understanding-LSTMs/) `article:medium`
- Stanford cs224n: Deep NLP (https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6) `course:medium` (replaces cs224d)
- TensorFlow Tutorials (https://www.tensorflow.org/tutorials/word2vec) `tutorial:medium` (start at Word2Vec + next 2 pages)
- The usual DL resources (pick one):
** Deep Learning Book (http://amzn.to/2tXgCiT) (Free HTML version (http://www.deeplearningbook.org/)) `book:hard` comprehensive DL bible; highly mathematical
** Fast.ai (http://course.fast.ai/) `course:medium` practical DL for coders


## Episode

RNN Review
** Vanilla: When words + running context is sufficient. 
  ** POS, NER, stocks, weather
** Bidirectional RNN (BiLSTM): When stuff from right helps too
** Encoder/decoder or Seq2seq: When you should hear everything first / spin a different way
  ** Classification, sentiment, translation
** Now w/ word embeddings

Train: backprop through time
** Vanishing/exploding gradient

LSTMs (http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
** ReLU vs Sigmoid vs TanH (Nonlinearities future episode)
** Forget gate layer
** Input gate layer: decides which values to update
** Tanh layer: creates new candidate values
** Output layer</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>1346120e3e578b15c8f34b31bc21ef78</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>RNN review, bi-directional RNNs, LSTM &amp; GRU cells.</itunes:subtitle>
<itunes:summary>RNN review, bi-directional RNNs, LSTM &amp; GRU cells.

## Resources
- Overview Articles: 
** Unreasonable Effectiveness of RNNs (http://karpathy.github.io/2015/05/21/rnn-effectiveness/) `article:easy`
** Deep Learning, NLP, and Representations (http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/) `article:medium`
** Understanding LSTM Networks (http://colah.github.io/posts/2015-08-Understanding-LSTMs/) `article:medium`
- Stanford cs224n: Deep NLP (https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6) `course:medium` (replaces cs224d)
- TensorFlow Tutorials (https://www.tensorflow.org/tutorials/word2vec) `tutorial:medium` (start at Word2Vec + next 2 pages)
- The usual DL resources (pick one):
** Deep Learning Book (http://amzn.to/2tXgCiT) (Free HTML version (http://www.deeplearningbook.org/)) `book:hard` comprehensive DL bible; highly mathematical
** Fast.ai (http://course.fast.ai/) `course:medium` practical DL for coders


## Episode

RNN Review
** Vanilla: When words + running context is sufficient. 
  ** POS, NER, stocks, weather
** Bidirectional RNN (BiLSTM): When stuff from right helps too
** Encoder/decoder or Seq2seq: When you should hear everything first / spin a different way
  ** Classification, sentiment, translation
** Now w/ word embeddings

Train: backprop through time
** Vanishing/exploding gradient

LSTMs (http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
** ReLU vs Sigmoid vs TanH (Nonlinearities future episode)
** Forget gate layer
** Input gate layer: decides which values to update
** Tanh layer: creates new candidate values
** Output layer</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>22. Deep NLP 1</title>
<link>undefined</link>
<pubDate>Fri, 28 Jul 2017 00:00:00 EST</pubDate>
<description>Recurrent Neural Networks (RNNs) and Word2Vec.

## Resources
- Overview Articles: 
** Unreasonable Effectiveness of RNNs (http://karpathy.github.io/2015/05/21/rnn-effectiveness/) `article:easy`
** Deep Learning, NLP, and Representations (http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/) `article:medium`
** Understanding LSTM Networks (http://colah.github.io/posts/2015-08-Understanding-LSTMs/) `article:medium`
- Stanford cs224n: Deep NLP (https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6) `course:medium` (replaces cs224d)
- TensorFlow Tutorials (https://www.tensorflow.org/tutorials/word2vec) `tutorial:medium` (start at Word2Vec + next 2 pages)
- Deep Learning Resources (http://ocdevel.com/podcasts/machine-learning/9)


## Episode

Deep NLP pros
- Language complexity &amp; nuances
** Feature engineering / learning
** Salary = degree*field, not +
** Multiple layers: pixels =&gt; lines =&gt; objects
** Multiple layers of language
- Once model to rule them all; E2E models

Sequence vs non-sequence
- DNN = ANN = MLP = Feed Forward
- RNNs for sequence (time series)

RNNs
- Looped hidden layers, learns nuances by combined features
- Carries info through time: language model
- Translation, sentiment, classification, POS, NER, ...
- Seq2seq, encode/decode

Word2Vec (https://www.tensorflow.org/tutorials/word2vec)
- One-hot (sparse) doesn't help (plus sparse = compute)
- Word embeddings
** Euclidean distance for synonyms / similar, Cosine for "projections" . king + queen - man = woman
** t-SNE (t-distributed stochastic neighbor embedding)
- Vector Space Models (VSMs). Learn from context, predictive vs count-based
- Predictive methods (neural probabilistic language models) - Learn model parameters which predict contexts
** Word2vec
** CBOW / Skip-Gram (cbow predicts center from context, skip-gram context from center. Small v large datasets)
** DNN, Softmax hypothesis fn, NCE loss (noise contrastive estimation)
- Count-based methods / Distributional Semantics - (compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word)
** GloVe
** Linear algebra stuff (PCA, LSA, SVD)
** Pros (?): faster, more accurate, incremental fitting. Cons (?): data hungry, more RAM. More info (http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/)
- DNN for POS, NER (or RNNs)</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>d9e15cfe501a8f0c6e3c075c09f7e682</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>Recurrent Neural Networks (RNNs) and Word2Vec.</itunes:subtitle>
<itunes:summary>Recurrent Neural Networks (RNNs) and Word2Vec.

## Resources
- Overview Articles: 
** Unreasonable Effectiveness of RNNs (http://karpathy.github.io/2015/05/21/rnn-effectiveness/) `article:easy`
** Deep Learning, NLP, and Representations (http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/) `article:medium`
** Understanding LSTM Networks (http://colah.github.io/posts/2015-08-Understanding-LSTMs/) `article:medium`
- Stanford cs224n: Deep NLP (https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6) `course:medium` (replaces cs224d)
- TensorFlow Tutorials (https://www.tensorflow.org/tutorials/word2vec) `tutorial:medium` (start at Word2Vec + next 2 pages)
- Deep Learning Resources (http://ocdevel.com/podcasts/machine-learning/9)


## Episode

Deep NLP pros
- Language complexity &amp; nuances
** Feature engineering / learning
** Salary = degree*field, not +
** Multiple layers: pixels =&gt; lines =&gt; objects
** Multiple layers of language
- Once model to rule them all; E2E models

Sequence vs non-sequence
- DNN = ANN = MLP = Feed Forward
- RNNs for sequence (time series)

RNNs
- Looped hidden layers, learns nuances by combined features
- Carries info through time: language model
- Translation, sentiment, classification, POS, NER, ...
- Seq2seq, encode/decode

Word2Vec (https://www.tensorflow.org/tutorials/word2vec)
- One-hot (sparse) doesn't help (plus sparse = compute)
- Word embeddings
** Euclidean distance for synonyms / similar, Cosine for "projections" . king + queen - man = woman
** t-SNE (t-distributed stochastic neighbor embedding)
- Vector Space Models (VSMs). Learn from context, predictive vs count-based
- Predictive methods (neural probabilistic language models) - Learn model parameters which predict contexts
** Word2vec
** CBOW / Skip-Gram (cbow predicts center from context, skip-gram context from center. Small v large datasets)
** DNN, Softmax hypothesis fn, NCE loss (noise contrastive estimation)
- Count-based methods / Distributional Semantics - (compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word)
** GloVe
** Linear algebra stuff (PCA, LSA, SVD)
** Pros (?): faster, more accurate, incremental fitting. Cons (?): data hungry, more RAM. More info (http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/)
- DNN for POS, NER (or RNNs)</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>21. Update</title>
<link>undefined</link>
<pubDate>Thu, 27 Jul 2017 00:00:00 EST</pubDate>
<description>Update on Patreon and resources.

Keep the podcast alive, donate on Patreon (https://www.patreon.com/machinelearningguide)</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>30566e73f0346fd82c174e2931a39a97</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>Update on Patreon and resources.</itunes:subtitle>
<itunes:summary>Update on Patreon and resources.

Keep the podcast alive, donate on Patreon (https://www.patreon.com/machinelearningguide)</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>20. Natural Language Processing 3</title>
<link>undefined</link>
<pubDate>Sun, 23 Jul 2017 00:00:00 EST</pubDate>
<description>Natural Language Processing classical/shallow algorithms.

## Resources
- Speech and Language Processing (http://amzn.to/2uZaNyg) `book:hard` comprehensive classical-NLP bible
- Stanford NLP YouTube (https://www.youtube.com/playlist?list=PL6397E4B26D00A269) `course|audio:medium`
- NLTK Book (http://www.nltk.org/book) `book:medium`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`

## Episode
- Parsing
** Constituents
** Grammar: Context Free Grammars (CFGs), Probabalistic CFGs (PCFGs), Cocke–Younger–Kasami (CYK)
** Dependency Tree: Greedy transition-based parsing (stack/buffer)
** SyntaxNet (English = Parsey McParseface)
- Relationship Extraction
- Question Answering / Textual Entailment (TF-IDF+Cosine Similarity; Parsing; NER)
- Automatic summarization (TF-IDF; TextRank)
- Machine Translation (details here (https://www.youtube.com/watch?v=QuELiw8tbx8&amp;list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&amp;index=9))</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>556b3779a8f8546de9457002a19e63b2</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>Natural Language Processing classical/shallow algorithms.</itunes:subtitle>
<itunes:summary>Natural Language Processing classical/shallow algorithms.

## Resources
- Speech and Language Processing (http://amzn.to/2uZaNyg) `book:hard` comprehensive classical-NLP bible
- Stanford NLP YouTube (https://www.youtube.com/playlist?list=PL6397E4B26D00A269) `course|audio:medium`
- NLTK Book (http://www.nltk.org/book) `book:medium`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`

## Episode
- Parsing
** Constituents
** Grammar: Context Free Grammars (CFGs), Probabalistic CFGs (PCFGs), Cocke–Younger–Kasami (CYK)
** Dependency Tree: Greedy transition-based parsing (stack/buffer)
** SyntaxNet (English = Parsey McParseface)
- Relationship Extraction
- Question Answering / Textual Entailment (TF-IDF+Cosine Similarity; Parsing; NER)
- Automatic summarization (TF-IDF; TextRank)
- Machine Translation (details here (https://www.youtube.com/watch?v=QuELiw8tbx8&amp;list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&amp;index=9))</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>19. Natural Language Processing 2</title>
<link>undefined</link>
<pubDate>Mon, 10 Jul 2017 00:00:00 EST</pubDate>
<description>Natural Language Processing classical/shallow algorithms.

## Resources
- Speech and Language Processing (http://amzn.to/2uZaNyg) `book:hard` comprehensive classical-NLP bible
- Stanford NLP YouTube (https://www.youtube.com/playlist?list=PL6397E4B26D00A269) `course|audio:medium`
- NLTK Book (http://www.nltk.org/book) `book:medium`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`

## Episode

- Edit distance: Levenshtein distance
- Stemming/lemmatization: Porter Stemmer
- N-grams, Tokens: regex
- Language models
** Machine translation, spelling correction, speech recognition
- Classification / Sentiment Analysis: SVM, Navie bayes
- Information Extraction (POS, NER): Models: MaxEnt, Hidden Markov Models (HMM), Conditional Random Fields (CRF)
- Generative vs Discriminitive models
** Generative: HMM, Bayes, LDA
** Discriminative: SVMs, MaxEnt / LogReg, ANNs
** Pros/Cons
  ** Generative depends on fewer data (NLP tends to be few data)
  ** MaxEnt vs Naive Bayes: Independence assumption of Bayes, etc ("Hong" "Kong")
- Topic Modeling and keyword extraction: Latent Dirichlet Allocation (LDA)
** LDA ~= LSA ~= LSI: Latent diriclet allocation, latent semantic indexing, latent semantic analysis
- Search / relevance / document-similarity: Bag-of-words, TF-IDF
- Similarity: Jaccard, Cosine, Euclidean</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>e05e640ba2f99105f52c4eef0c5cabfb</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>Natural Language Processing classical/shallow algorithms.</itunes:subtitle>
<itunes:summary>Natural Language Processing classical/shallow algorithms.

## Resources
- Speech and Language Processing (http://amzn.to/2uZaNyg) `book:hard` comprehensive classical-NLP bible
- Stanford NLP YouTube (https://www.youtube.com/playlist?list=PL6397E4B26D00A269) `course|audio:medium`
- NLTK Book (http://www.nltk.org/book) `book:medium`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`

## Episode

- Edit distance: Levenshtein distance
- Stemming/lemmatization: Porter Stemmer
- N-grams, Tokens: regex
- Language models
** Machine translation, spelling correction, speech recognition
- Classification / Sentiment Analysis: SVM, Navie bayes
- Information Extraction (POS, NER): Models: MaxEnt, Hidden Markov Models (HMM), Conditional Random Fields (CRF)
- Generative vs Discriminitive models
** Generative: HMM, Bayes, LDA
** Discriminative: SVMs, MaxEnt / LogReg, ANNs
** Pros/Cons
  ** Generative depends on fewer data (NLP tends to be few data)
  ** MaxEnt vs Naive Bayes: Independence assumption of Bayes, etc ("Hong" "Kong")
- Topic Modeling and keyword extraction: Latent Dirichlet Allocation (LDA)
** LDA ~= LSA ~= LSI: Latent diriclet allocation, latent semantic indexing, latent semantic analysis
- Search / relevance / document-similarity: Bag-of-words, TF-IDF
- Similarity: Jaccard, Cosine, Euclidean</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>18. Natural Language Processing 1</title>
<link>undefined</link>
<pubDate>Sun, 25 Jun 2017 00:00:00 EST</pubDate>
<description>Introduction to Natural Language Processing (NLP) topics.

## Resources
- Speech and Language Processing (http://amzn.to/2uZaNyg) `book:hard` comprehensive classical-NLP bible
- Stanford NLP YouTube (https://www.youtube.com/playlist?list=PL6397E4B26D00A269) `course|audio:medium`
- NLTK Book (http://www.nltk.org/book) `book:medium`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`

## Errata
22:21 "cat &amp; car different by one word" should be "different by one letter"

## Episode
Syntax vs Semantics

Parts
- Corpus
- Lexicon
- Morphology
** Lemmas &amp; Stems (reduce morphological variation; lemmatization more sophisticated)
** Tokens
** Stop words
** Edit-distance
** Word sense disambiguation

Syntax / Tasks
- Info Extraction (POS, NER, Relationship extraction)
- Parsing

Goals
- Spell check
- Classification
** Tagging (topic modeling / keyword extraction)
** Sentiment analysis
- Search / relevance, document similarity
- Natural language understanding
** Question answering
** Textual entailment
** Machine Translation (AI-complete)
** NLU vs NLP
- Natural language generation
** Image captioning
** Chatbots
** Automatic summarization
- Won't cover
** Optical character recognition (OCR)
** Speech (TTS, STT, Segmentation, Diarization)</description>
<enclosure url="undefined" length="undefined" type="audio/mpeg"/>
<guid>d8ebdbe6640d0d34f12778f90b91db8d</guid>
<itunes:duration>undefined</itunes:duration>
<itunes:subtitle>Introduction to Natural Language Processing (NLP) topics.</itunes:subtitle>
<itunes:summary>Introduction to Natural Language Processing (NLP) topics.

## Resources
- Speech and Language Processing (http://amzn.to/2uZaNyg) `book:hard` comprehensive classical-NLP bible
- Stanford NLP YouTube (https://www.youtube.com/playlist?list=PL6397E4B26D00A269) `course|audio:medium`
- NLTK Book (http://www.nltk.org/book) `book:medium`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`

## Errata
22:21 "cat &amp; car different by one word" should be "different by one letter"

## Episode
Syntax vs Semantics

Parts
- Corpus
- Lexicon
- Morphology
** Lemmas &amp; Stems (reduce morphological variation; lemmatization more sophisticated)
** Tokens
** Stop words
** Edit-distance
** Word sense disambiguation

Syntax / Tasks
- Info Extraction (POS, NER, Relationship extraction)
- Parsing

Goals
- Spell check
- Classification
** Tagging (topic modeling / keyword extraction)
** Sentiment analysis
- Search / relevance, document similarity
- Natural language understanding
** Question answering
** Textual entailment
** Machine Translation (AI-complete)
** NLU vs NLP
- Natural language generation
** Image captioning
** Chatbots
** Automatic summarization
- Won't cover
** Optical character recognition (OCR)
** Speech (TTS, STT, Segmentation, Diarization)</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>17. Checkpoint</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-17.mp3</link>
<pubDate>Sun, 04 Jun 2017 00:00:00 EST</pubDate>
<description>Checkpoint - learn the material offline!

45m/d ML
- Coursera (https://www.coursera.org/learn/machine-learning) `course:hard`
- Python (http://amzn.to/2mVgtJW) `book:medium`
- Deep Learning Resources (http://ocdevel.com/podcasts/machine-learning/9)
- Go deeper on shallow algos
** Elements of Statistical Learning (http://amzn.to/2tWW8He) `book:hard`
** Pattern Recognition and Machine Learning (http://amzn.to/2sDIIfb) (Free PDF? (https://goo.gl/aX038j)) `book:hard`

15m/d Math
- Either LinAlg (https://www.khanacademy.org/math/linear-algebra) `course:medium` OR Fast.ai (http://www.fast.ai/2017/07/17/num-lin-alg/) `course:medium`
- Stats (https://www.khanacademy.org/math/statistics-probability) `course:medium`
- Calc (https://www.khanacademy.org/math/calculus-home) `course:medium`

Audio
- (removed CS229 - very heavy chalkboard use lends poorly to audio)
- The Master Algorithm (http://amzn.to/2kLOQjW) `audio:medium` Semi-technical overview of ML basics &amp; main algorithms
- Mathematical Decision Making (https://goo.gl/V75I49) `audio|course:hard` course on "Operations Research", similar to ML
- Statistics (https://goo.gl/4vvXJs), Probability (https://goo.gl/Q4KwZ6) `audio|course:hard`
- Calculus 1 (https://goo.gl/fcLP3l), 2 (https://goo.gl/sBpljN), 3 (https://goo.gl/8Hdwuh) `audio|course:hard`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`

Kaggle.com (https://www.kaggle.com/)
</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-17.mp3" length="6625180" type="audio/mpeg"/>
<guid>4977e285-d4fc-45cb-b3a5-aed9e97915c2</guid>
<itunes:duration>6:59</itunes:duration>
<itunes:subtitle>Checkpoint - learn the material offline!</itunes:subtitle>
<itunes:summary>Checkpoint - learn the material offline!

45m/d ML
- Coursera (https://www.coursera.org/learn/machine-learning) `course:hard`
- Python (http://amzn.to/2mVgtJW) `book:medium`
- Deep Learning Resources (http://ocdevel.com/podcasts/machine-learning/9)
- Go deeper on shallow algos
** Elements of Statistical Learning (http://amzn.to/2tWW8He) `book:hard`
** Pattern Recognition and Machine Learning (http://amzn.to/2sDIIfb) (Free PDF? (https://goo.gl/aX038j)) `book:hard`

15m/d Math
- Either LinAlg (https://www.khanacademy.org/math/linear-algebra) `course:medium` OR Fast.ai (http://www.fast.ai/2017/07/17/num-lin-alg/) `course:medium`
- Stats (https://www.khanacademy.org/math/statistics-probability) `course:medium`
- Calc (https://www.khanacademy.org/math/calculus-home) `course:medium`

Audio
- (removed CS229 - very heavy chalkboard use lends poorly to audio)
- The Master Algorithm (http://amzn.to/2kLOQjW) `audio:medium` Semi-technical overview of ML basics &amp; main algorithms
- Mathematical Decision Making (https://goo.gl/V75I49) `audio|course:hard` course on "Operations Research", similar to ML
- Statistics (https://goo.gl/4vvXJs), Probability (https://goo.gl/Q4KwZ6) `audio|course:hard`
- Calculus 1 (https://goo.gl/fcLP3l), 2 (https://goo.gl/sBpljN), 3 (https://goo.gl/8Hdwuh) `audio|course:hard`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`

Kaggle.com (https://www.kaggle.com/)
</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>16. Consciousness</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-16.mp3</link>
<pubDate>Sun, 21 May 2017 00:00:00 EST</pubDate>
<description>Can AI be conscious?

## Resources

- Philosophy of Mind: Brains, Consciousness, and Thinking Machines (Audible (http://amzn.to/2kQGgk5), TGC (https://goo.gl/fDteyi)) `audio:easy`

## Episode

Inspirations for AI
- economic automation
- singularity
- consciousness

Definitinitions
- cogsci: neuroscience, neuro-x(biology, physiology, computational __, etc), psychology, philosophy, AI
** computational neuroscience =&gt; perceptron
** frank rosenblatt, warren McCulloche, walter pitts - all brain guys (neurobiology, neurophysiology, computational neuroscience respectively)
- intelligence (computation) vs consciousness (soul); intelligence in scale (animals); brain in scale; consciousness in scale?
- perception, self-identity, memory, attention; (self reflection is just a human-special component)
- awereness (qualia / sentience / subjective experience); modified by attention? (driving, dreams, coma)
- missing: emotions; just built-in goal reinforcemer. plus we don't know how machines experience reinforcement (floor-is-lava)

Hard vs soft problem
** soft problem = neuroscience
** hard problem = philosophy
  ** dualism: pineal gland, issue with physical-&gt;metaphysical; society of mind / connected intelligences
  ** maybe definitively non-science, since subjective
  ** maybe matter of time; phil is pre-science at each juncture; science turns magic =&gt; known (sickness). Either hard problem is unscientific (phil) or around the corner

Emergence (emergent property)

Computational theory of mind
- intelligence &amp; consciousness connected / same
- think: word2vec = understanding? 
- consciousness in scale; does this mean every layer has its own consciousness? Panpsychism. I don't know - just concerned with that which does exhibit intelligence
- integrated information theory
- freewill; conscious / awareness center activated after decision made; all the information in place before whole ; westworld

Biological plausibility
- planes, brains
- sans bio-plaus, functionalism; zombies; turing test; searle's chinese room</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-16.mp3" length="69807705" type="audio/mpeg"/>
<guid>c2db5df8-936b-4404-8f0f-7eb188bfe9ab</guid>
<itunes:duration>01:14:57</itunes:duration>
<itunes:subtitle>Can AI be conscious?</itunes:subtitle>
<itunes:summary>Can AI be conscious?

## Resources

- Philosophy of Mind: Brains, Consciousness, and Thinking Machines (Audible (http://amzn.to/2kQGgk5), TGC (https://goo.gl/fDteyi)) `audio:easy`

## Episode

Inspirations for AI
- economic automation
- singularity
- consciousness

Definitinitions
- cogsci: neuroscience, neuro-x(biology, physiology, computational __, etc), psychology, philosophy, AI
** computational neuroscience =&gt; perceptron
** frank rosenblatt, warren McCulloche, walter pitts - all brain guys (neurobiology, neurophysiology, computational neuroscience respectively)
- intelligence (computation) vs consciousness (soul); intelligence in scale (animals); brain in scale; consciousness in scale?
- perception, self-identity, memory, attention; (self reflection is just a human-special component)
- awereness (qualia / sentience / subjective experience); modified by attention? (driving, dreams, coma)
- missing: emotions; just built-in goal reinforcemer. plus we don't know how machines experience reinforcement (floor-is-lava)

Hard vs soft problem
** soft problem = neuroscience
** hard problem = philosophy
  ** dualism: pineal gland, issue with physical-&gt;metaphysical; society of mind / connected intelligences
  ** maybe definitively non-science, since subjective
  ** maybe matter of time; phil is pre-science at each juncture; science turns magic =&gt; known (sickness). Either hard problem is unscientific (phil) or around the corner

Emergence (emergent property)

Computational theory of mind
- intelligence &amp; consciousness connected / same
- think: word2vec = understanding? 
- consciousness in scale; does this mean every layer has its own consciousness? Panpsychism. I don't know - just concerned with that which does exhibit intelligence
- integrated information theory
- freewill; conscious / awareness center activated after decision made; all the information in place before whole ; westworld

Biological plausibility
- planes, brains
- sans bio-plaus, functionalism; zombies; turing test; searle's chinese room</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>15. Performance</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-15.mp3</link>
<pubDate>Sun, 07 May 2017 00:00:00 EST</pubDate>
<description>Performance evaluation &amp; improvement

## Episode

Performance evaluation

- Performance measures: accuracy, precision, recall, F1/F2 score
- Cross validation: split your data into train, validation, test sets
- Training set is for training your algorithm
- Validation set is to test your algorithm's performance. It can be used to inform changing your model (ie, hyperparameters)
- Test set is used for your final score. It can't be used to inform changing your model.

Performance improvement
 
- Modify hyperpamaraters
- Data: collect more, fill in missing cells, normalize fields
- Regularize: reduce overfitting (high variance) and underfitting (high bias)
</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-15.mp3" length="37982381" type="audio/mpeg"/>
<guid>7da253aa-b035-4702-8475-55b8d3eeeebd</guid>
<itunes:duration>41:24</itunes:duration>
<itunes:subtitle>Performance evaluation &amp; improvement</itunes:subtitle>
<itunes:summary>Performance evaluation &amp; improvement

## Episode

Performance evaluation

- Performance measures: accuracy, precision, recall, F1/F2 score
- Cross validation: split your data into train, validation, test sets
- Training set is for training your algorithm
- Validation set is to test your algorithm's performance. It can be used to inform changing your model (ie, hyperparameters)
- Test set is used for your final score. It can't be used to inform changing your model.

Performance improvement
 
- Modify hyperpamaraters
- Data: collect more, fill in missing cells, normalize fields
- Regularize: reduce overfitting (high variance) and underfitting (high bias)
</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>14. Shallow Algos 3</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-14.mp3</link>
<pubDate>Sun, 23 Apr 2017 00:00:00 EST</pubDate>
<description>Speed run of Anomaly Detection, Recommenders(Content Filtering vs Collaborative Filtering), and Markov Chain Monte Carlo (MCMC)

## Resources
- Andrew Ng Week 9 (https://www.coursera.org/learn/machine-learning/resources/szFCa)

## Episode
- Anomoly Detection algorithm
- Recommender Systems (Content Filtering, Collaborative Filtering)
- Markov Chains &amp; Monte Carlo</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-14.mp3" length="45705749" type="audio/mpeg"/>
<guid>8ec3010c-8897-43fa-b90b-14f7e43912a8</guid>
<itunes:duration>48:06</itunes:duration>
<itunes:subtitle>Speed run of Anomaly Detection, Recommenders(Content Filtering vs Collaborative Filtering), and Markov Chain Monte Carlo (MCMC)</itunes:subtitle>
<itunes:summary>Speed run of Anomaly Detection, Recommenders(Content Filtering vs Collaborative Filtering), and Markov Chain Monte Carlo (MCMC)

## Resources
- Andrew Ng Week 9 (https://www.coursera.org/learn/machine-learning/resources/szFCa)

## Episode
- Anomoly Detection algorithm
- Recommender Systems (Content Filtering, Collaborative Filtering)
- Markov Chains &amp; Monte Carlo</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>13. Shallow Algos 2</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-13.mp3</link>
<pubDate>Sun, 09 Apr 2017 00:00:00 EST</pubDate>
<description>Speed run of Support Vector Machines (SVMs) and Naive Bayes Classifier.

## Resources
- Andrew Ng Week 7 (https://www.coursera.org/learn/machine-learning/resources/Es9Qo)
- Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium` (replaced R book)
- Mathematical Decision Making (https://goo.gl/V75I49) `audio|course:hard` course on "Operations Research", similar to ML
- Which algo to use?
** Pros/cons table for algos (https://blog.recast.ai/machine-learning-algorithms/2/) `picture`
** Decision tree of algos (http://scikit-learn.org/stable/tutorial/machine_learning_map/) `picture`

## Episode
- Support Vector Machines (SVM)
- Naive Bayes Classifier</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-13.mp3" length="51788056" type="audio/mpeg"/>
<guid>af4c231e-c8c1-4d91-ab21-2e256669982e</guid>
<itunes:duration>55:12</itunes:duration>
<itunes:subtitle>Speed run of Support Vector Machines (SVMs) and Naive Bayes Classifier.</itunes:subtitle>
<itunes:summary>Speed run of Support Vector Machines (SVMs) and Naive Bayes Classifier.

## Resources
- Andrew Ng Week 7 (https://www.coursera.org/learn/machine-learning/resources/Es9Qo)
- Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium` (replaced R book)
- Mathematical Decision Making (https://goo.gl/V75I49) `audio|course:hard` course on "Operations Research", similar to ML
- Which algo to use?
** Pros/cons table for algos (https://blog.recast.ai/machine-learning-algorithms/2/) `picture`
** Decision tree of algos (http://scikit-learn.org/stable/tutorial/machine_learning_map/) `picture`

## Episode
- Support Vector Machines (SVM)
- Naive Bayes Classifier</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>12. Shallow Algos 1</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-12.mp3</link>
<pubDate>Sun, 19 Mar 2017 00:00:00 EST</pubDate>
<description>Speed-run of some shallow algorithms: K Nearest Neighbors (KNN); K-means; Apriori; PCA; Decision Trees

## Resources
- Andrew Ng Week 8 (https://www.coursera.org/learn/machine-learning/resources/kGWsY)
- Tour of Machine Learning Algorithms (http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms) `article:easy`
- Elements of Statistical Learning (http://amzn.to/2tWW8He) `book:hard`
- Pattern Recognition and Machine Learning (http://amzn.to/2sDIIfb) (Free PDF? (https://goo.gl/aX038j)) `book:hard`
- Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium` (replaced R book)
- Which algo to use?
** Pros/cons table for algos (https://blog.recast.ai/machine-learning-algorithms/2/) `picture`
** Decision tree of algos (http://scikit-learn.org/stable/tutorial/machine_learning_map/) `picture`

## Episode
KNN (supervised)

Unsupervised
- Clustering -&gt; K-Means
- Association rule learning / Market basket -&gt; Apriori
- Dimensionality Reduction -&gt; PCA

Decision Trees (supervised, classify/regress)
- Random Forests
- Gradient Boost</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-12.mp3" length="50030574" type="audio/mpeg"/>
<guid>1074a375-6831-456d-9bbc-d28c8f85a557</guid>
<itunes:duration>53:17</itunes:duration>
<itunes:subtitle>Speed-run of some shallow algorithms: K Nearest Neighbors (KNN); K-means; Apriori; PCA; Decision Trees</itunes:subtitle>
<itunes:summary>Speed-run of some shallow algorithms: K Nearest Neighbors (KNN); K-means; Apriori; PCA; Decision Trees

## Resources
- Andrew Ng Week 8 (https://www.coursera.org/learn/machine-learning/resources/kGWsY)
- Tour of Machine Learning Algorithms (http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms) `article:easy`
- Elements of Statistical Learning (http://amzn.to/2tWW8He) `book:hard`
- Pattern Recognition and Machine Learning (http://amzn.to/2sDIIfb) (Free PDF? (https://goo.gl/aX038j)) `book:hard`
- Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium` (replaced R book)
- Which algo to use?
** Pros/cons table for algos (https://blog.recast.ai/machine-learning-algorithms/2/) `picture`
** Decision tree of algos (http://scikit-learn.org/stable/tutorial/machine_learning_map/) `picture`

## Episode
KNN (supervised)

Unsupervised
- Clustering -&gt; K-Means
- Association rule learning / Market basket -&gt; Apriori
- Dimensionality Reduction -&gt; PCA

Decision Trees (supervised, classify/regress)
- Random Forests
- Gradient Boost</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>11. Checkpoint</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-11.mp3</link>
<pubDate>Wed, 08 Mar 2017 00:00:00 EST</pubDate>
<description>Checkpoint - start learning the material offline!

45m/d ML
- Coursera (https://www.coursera.org/learn/machine-learning) `course:hard`
- Python (http://amzn.to/2mVgtJW) `book:medium`
- Deep Learning Resources (http://ocdevel.com/podcasts/machine-learning/9)

15m/d Math (KhanAcademy)
- Either LinAlg (https://www.khanacademy.org/math/linear-algebra) `course:medium` OR Fast.ai (http://www.fast.ai/2017/07/17/num-lin-alg/) `course:medium`
- Stats (https://www.khanacademy.org/math/statistics-probability) `course:medium`
- Calc (https://www.khanacademy.org/math/calculus-home) `course:medium`

Audio
- The Master Algorithm (http://amzn.to/2kLOQjW) `audio:medium` Semi-technical overview of ML basics &amp; main algorithms
- Mathematical Decision Making (https://goo.gl/V75I49) `audio|course:hard` course on "Operations Research", similar to ML
- Statistics (https://goo.gl/4vvXJs), Probability (https://goo.gl/Q4KwZ6) `audio|course:hard`
- Calculus 1 (https://goo.gl/fcLP3l), 2 (https://goo.gl/sBpljN), 3 (https://goo.gl/8Hdwuh) `audio|course:hard`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`
</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-11.mp3" length="6946229" type="audio/mpeg"/>
<guid>fe205bbc-b9d4-4df5-b840-c6f5b728903f</guid>
<itunes:duration>7:45</itunes:duration>
<itunes:subtitle>Checkpoint - start learning the material offline!</itunes:subtitle>
<itunes:summary>Checkpoint - start learning the material offline!

45m/d ML
- Coursera (https://www.coursera.org/learn/machine-learning) `course:hard`
- Python (http://amzn.to/2mVgtJW) `book:medium`
- Deep Learning Resources (http://ocdevel.com/podcasts/machine-learning/9)

15m/d Math (KhanAcademy)
- Either LinAlg (https://www.khanacademy.org/math/linear-algebra) `course:medium` OR Fast.ai (http://www.fast.ai/2017/07/17/num-lin-alg/) `course:medium`
- Stats (https://www.khanacademy.org/math/statistics-probability) `course:medium`
- Calc (https://www.khanacademy.org/math/calculus-home) `course:medium`

Audio
- The Master Algorithm (http://amzn.to/2kLOQjW) `audio:medium` Semi-technical overview of ML basics &amp; main algorithms
- Mathematical Decision Making (https://goo.gl/V75I49) `audio|course:hard` course on "Operations Research", similar to ML
- Statistics (https://goo.gl/4vvXJs), Probability (https://goo.gl/Q4KwZ6) `audio|course:hard`
- Calculus 1 (https://goo.gl/fcLP3l), 2 (https://goo.gl/sBpljN), 3 (https://goo.gl/8Hdwuh) `audio|course:hard`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`
</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>10. Languages &amp; Frameworks</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-10.mp3</link>
<pubDate>Tue, 07 Mar 2017 00:00:00 EST</pubDate>
<description>Languages &amp; frameworks comparison. Languages: Python, R, MATLAB/Octave, Julia, Java/Scala, C/C++. Frameworks: Hadoop/Spark, Deeplearning4J, Theano, Torch, TensorFlow.

## Resources
- Python (http://amzn.to/2mVgtJW) `book:medium`
- TensorFlow Tutorials (https://www.tensorflow.org/get_started/get_started) `tutorial:medium`
- Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium`

## Episode
Languages
- C/C++
** Performance
** GPU (CUDA/cuDNN)
- Math Langs
** R
** MATLAB / Octave
** Julia
- Java / Scala
** Data mining
** Hadoop + Mahout / Spark + SparkML
** Deeplearning4j
- Python
** R =&gt; Pandas
** MATLAB =&gt; numpy
** C/C++/GPU =&gt; TensorFlow (or other symbolic graph)
** Data Mining =&gt; PySpark
** Server (Flask, Django)
- Analogy: Data =&gt; Analytics (biz intelligence, etc) =&gt; Adsense
- Other languages like Node, Go, Rust (forgot to mention) see my answer (https://goo.gl/9d21xE) for why NOT to use them.
- Articles
** Best Programming Language for Machine Learning (http://machinelearningmastery.com/best-programming-language-for-machine-learning)
** Data Science Job Report 2017 (http://r4stats.com/2017/02/28/r-passes-sas)
  
Frameworks
- ML libraries
** Numpy, Pandas, scikit-learn
- Computational/symbolic graphs
** Automatic differentiation
- Theano
** Math layer
** Blocks/Lasagne ML layer
** Keras DL layer
- Torch
** CNNs
** note about RNNs
- TensorFlow
** Perf over time
** Mobile etc
** Keras
- Others
** Caffe (old-n-dying, C++)
** CNTK (MS)
** mxnet (Amazon)
** DL4J
** OpenCV (vision only)
- Articles
** An Overview of Python Deep Learning Frameworks (http://www.kdnuggets.com/2017/02/python-deep-learning-frameworks-overview.html)
** Evaluation of Deep Learning Toolkits (https://github.com/zer0n/deepframeworks/blob/master/README.md)
** Comparing Frameworks: Deeplearning4j, Torch, Theano, TensorFlow, Caffe, Paddle, MxNet, Keras &amp; CNTK (https://deeplearning4j.org/compare-dl4j-torch7-pylearn) - grain of salt, it's super heavy DL4J propaganda (written by them)</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-10.mp3" length="39407399" type="audio/mpeg"/>
<guid>c613d746-0916-448e-8315-5ac4323389e2</guid>
<itunes:duration>44:17</itunes:duration>
<itunes:subtitle>Languages &amp; frameworks comparison. Languages: Python, R, MATLAB/Octave, Julia, Java/Scala, C/C++. Frameworks: Hadoop/Spark, Deeplearning4J, Theano, Torch, TensorFlow.</itunes:subtitle>
<itunes:summary>Languages &amp; frameworks comparison. Languages: Python, R, MATLAB/Octave, Julia, Java/Scala, C/C++. Frameworks: Hadoop/Spark, Deeplearning4J, Theano, Torch, TensorFlow.

## Resources
- Python (http://amzn.to/2mVgtJW) `book:medium`
- TensorFlow Tutorials (https://www.tensorflow.org/get_started/get_started) `tutorial:medium`
- Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium`

## Episode
Languages
- C/C++
** Performance
** GPU (CUDA/cuDNN)
- Math Langs
** R
** MATLAB / Octave
** Julia
- Java / Scala
** Data mining
** Hadoop + Mahout / Spark + SparkML
** Deeplearning4j
- Python
** R =&gt; Pandas
** MATLAB =&gt; numpy
** C/C++/GPU =&gt; TensorFlow (or other symbolic graph)
** Data Mining =&gt; PySpark
** Server (Flask, Django)
- Analogy: Data =&gt; Analytics (biz intelligence, etc) =&gt; Adsense
- Other languages like Node, Go, Rust (forgot to mention) see my answer (https://goo.gl/9d21xE) for why NOT to use them.
- Articles
** Best Programming Language for Machine Learning (http://machinelearningmastery.com/best-programming-language-for-machine-learning)
** Data Science Job Report 2017 (http://r4stats.com/2017/02/28/r-passes-sas)
  
Frameworks
- ML libraries
** Numpy, Pandas, scikit-learn
- Computational/symbolic graphs
** Automatic differentiation
- Theano
** Math layer
** Blocks/Lasagne ML layer
** Keras DL layer
- Torch
** CNNs
** note about RNNs
- TensorFlow
** Perf over time
** Mobile etc
** Keras
- Others
** Caffe (old-n-dying, C++)
** CNTK (MS)
** mxnet (Amazon)
** DL4J
** OpenCV (vision only)
- Articles
** An Overview of Python Deep Learning Frameworks (http://www.kdnuggets.com/2017/02/python-deep-learning-frameworks-overview.html)
** Evaluation of Deep Learning Toolkits (https://github.com/zer0n/deepframeworks/blob/master/README.md)
** Comparing Frameworks: Deeplearning4j, Torch, Theano, TensorFlow, Caffe, Paddle, MxNet, Keras &amp; CNTK (https://deeplearning4j.org/compare-dl4j-torch7-pylearn) - grain of salt, it's super heavy DL4J propaganda (written by them)</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>9. Deep Learning</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-9.mp3</link>
<pubDate>Sat, 04 Mar 2017 00:00:00 EST</pubDate>
<description>Deep learning and neural networks. How to stack our logisitic regression units into a multi-layer perceptron.

## Resources
- Overview: 
** Deep Learning Simplified (https://www.youtube.com/watch?v=b99UVkWzYTQ) `video:easy` quick series to get a lay-of-the-land.
- Quickstart:
** TensorFlow Tutorials (https://www.tensorflow.org/get_started/get_started) `tutorial:medium`
- Deep-dive code (pick one):
** Fast.ai (http://course.fast.ai/) `course:medium` practical DL for coders
** Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium`
- Deep-dive theory:
** Deep Learning Book (http://amzn.to/2tXgCiT) (Free HTML version (http://www.deeplearningbook.org/)) `book:hard` comprehensive DL bible; highly mathematical  

## Episode
- Value
** Represents brain? Magic black-box
** Feature learning (layer removed from programmer)
** Subsumes AI
- Stacked shallow learning
** Logistic regression = lego, Neural Network = castle
- Deep Learning =&gt; ANNs =&gt; MLPs (&amp; RNNs, CNNs, DQNs, etc)
** MLP: Perceptron vs LogReg / sigmoid activation
- Architecture
** (Feed forward) Input =&gt; Hidden Layers =&gt; Hypothesis fn
** "Feed forward" vs recursive (RNNs, later)
** (Loss function) Cross entropy
** (Learn) Back Propagation
- Price ~ smoking + obesity + age^2
** 1-layer MLP
- Face? ~ pixels
** Extra layer = hierarchical breakdown
** Inputs =&gt; Employees =&gt; Supervisors =&gt; Boss
- Backprop / Gradient descent
** Optimizers: adagrad, adam, ... vs gradient descent
- Silver bullet, but don't abuse
** linear (housing market)
** features don't combine
** expensive: like hiring a company when the boss h(x) does all the work
- Brian comparison (dentrites, axons); early pioneers as neuroscientists / cogsci
- Different types
** vs brain
** RNNs
** CNNs
- Activation fns
** Activation units / neurons (hidden layer)
** Relu, TanH, Sigmoid</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-9.mp3" length="45855231" type="audio/mpeg"/>
<guid>d842fe61-7cf2-4209-9cb3-d29be6c4d1a8</guid>
<itunes:duration>51:09</itunes:duration>
<itunes:subtitle>Deep learning and neural networks. How to stack our logisitic regression units into a multi-layer perceptron.</itunes:subtitle>
<itunes:summary>Deep learning and neural networks. How to stack our logisitic regression units into a multi-layer perceptron.

## Resources
- Overview: 
** Deep Learning Simplified (https://www.youtube.com/watch?v=b99UVkWzYTQ) `video:easy` quick series to get a lay-of-the-land.
- Quickstart:
** TensorFlow Tutorials (https://www.tensorflow.org/get_started/get_started) `tutorial:medium`
- Deep-dive code (pick one):
** Fast.ai (http://course.fast.ai/) `course:medium` practical DL for coders
** Hands-On Machine Learning with Scikit-Learn and TensorFlow (http://amzn.to/2tVdIXN) `book:medium`
- Deep-dive theory:
** Deep Learning Book (http://amzn.to/2tXgCiT) (Free HTML version (http://www.deeplearningbook.org/)) `book:hard` comprehensive DL bible; highly mathematical  

## Episode
- Value
** Represents brain? Magic black-box
** Feature learning (layer removed from programmer)
** Subsumes AI
- Stacked shallow learning
** Logistic regression = lego, Neural Network = castle
- Deep Learning =&gt; ANNs =&gt; MLPs (&amp; RNNs, CNNs, DQNs, etc)
** MLP: Perceptron vs LogReg / sigmoid activation
- Architecture
** (Feed forward) Input =&gt; Hidden Layers =&gt; Hypothesis fn
** "Feed forward" vs recursive (RNNs, later)
** (Loss function) Cross entropy
** (Learn) Back Propagation
- Price ~ smoking + obesity + age^2
** 1-layer MLP
- Face? ~ pixels
** Extra layer = hierarchical breakdown
** Inputs =&gt; Employees =&gt; Supervisors =&gt; Boss
- Backprop / Gradient descent
** Optimizers: adagrad, adam, ... vs gradient descent
- Silver bullet, but don't abuse
** linear (housing market)
** features don't combine
** expensive: like hiring a company when the boss h(x) does all the work
- Brian comparison (dentrites, axons); early pioneers as neuroscientists / cogsci
- Different types
** vs brain
** RNNs
** CNNs
- Activation fns
** Activation units / neurons (hidden layer)
** Relu, TanH, Sigmoid</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>8. Math</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-8.mp3</link>
<pubDate>Thu, 23 Feb 2017 00:00:00 EST</pubDate>
<description>Introduction to the branches of mathematics used in machine learning. Linear algebra, statistics, calculus.

## Resources
Come back here after you've finished Ng's course; or learn these resources in tandem with ML (say 1 day a week).

Primers (PDFs)
- See "Section Notes" of cs229 (http://cs229.stanford.edu/materials.html) `handout:medium`

KhanAcademy: 
- Either LinAlg (https://www.khanacademy.org/math/linear-algebra) `course:medium` OR Fast.ai (http://www.fast.ai/2017/07/17/num-lin-alg/) `course:medium`
- Stats (https://www.khanacademy.org/math/statistics-probability) `course:medium`
- Calc (https://www.khanacademy.org/math/calculus-home) `course:medium`

Books
- Linear Algebra Done Right (http://amzn.to/2t28p8F) `book:hard`
- All of statistics (http://amzn.to/2t2dOwg) `book:hard`
- Calculus (http://amzn.to/2tXfXhp) `book:hard`

Audio (supplementary material)
- Statistics (https://goo.gl/4vvXJs), Probability (https://goo.gl/Q4KwZ6) `audio|course:hard` 
- Calculus 1 (https://goo.gl/fcLP3l), 2 (https://goo.gl/sBpljN), 3 (https://goo.gl/8Hdwuh) `audio|course:hard`
- Mathematical Decision Making (https://goo.gl/V75I49) `audio|course:hard` course on "Operations Research", similar to ML
- Information Theory (https://goo.gl/ugAi2m) `audio|course:hard`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`

## Episode
- Linear Algebra = Matrix (or "Tensor") math. Wx + b. Chopping in our analogy.
- Stats = Probability/inference, the heart of machine learning. Recipes/cookbook.
- Calculus = Learning. Moving our error dot to the bottom of the valley. Baking, the actual "cook" step.</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-8.mp3" length="24852040" type="audio/mpeg"/>
<guid>a5c01d38-5242-4b63-b265-81fc53d38ad3</guid>
<itunes:duration>27:23</itunes:duration>
<itunes:subtitle>Introduction to the branches of mathematics used in machine learning. Linear algebra, statistics, calculus.</itunes:subtitle>
<itunes:summary>Introduction to the branches of mathematics used in machine learning. Linear algebra, statistics, calculus.

## Resources
Come back here after you've finished Ng's course; or learn these resources in tandem with ML (say 1 day a week).

Primers (PDFs)
- See "Section Notes" of cs229 (http://cs229.stanford.edu/materials.html) `handout:medium`

KhanAcademy: 
- Either LinAlg (https://www.khanacademy.org/math/linear-algebra) `course:medium` OR Fast.ai (http://www.fast.ai/2017/07/17/num-lin-alg/) `course:medium`
- Stats (https://www.khanacademy.org/math/statistics-probability) `course:medium`
- Calc (https://www.khanacademy.org/math/calculus-home) `course:medium`

Books
- Linear Algebra Done Right (http://amzn.to/2t28p8F) `book:hard`
- All of statistics (http://amzn.to/2t2dOwg) `book:hard`
- Calculus (http://amzn.to/2tXfXhp) `book:hard`

Audio (supplementary material)
- Statistics (https://goo.gl/4vvXJs), Probability (https://goo.gl/Q4KwZ6) `audio|course:hard` 
- Calculus 1 (https://goo.gl/fcLP3l), 2 (https://goo.gl/sBpljN), 3 (https://goo.gl/8Hdwuh) `audio|course:hard`
- Mathematical Decision Making (https://goo.gl/V75I49) `audio|course:hard` course on "Operations Research", similar to ML
- Information Theory (https://goo.gl/ugAi2m) `audio|course:hard`
- Convert video to audio:
** mp4 =&gt; mp3: `for f in *.mp4; do ffmpeg -i "$f" "${f%.mp4}.mp3" &amp;&amp; rm "$f"; done`
** youtube =&gt; mp3: setup youtube-dl (https://github.com/rg3/youtube-dl) and run `youtube-dl -x youtube.com/playlist?list=&lt;EDIT THIS&gt;`

## Episode
- Linear Algebra = Matrix (or "Tensor") math. Wx + b. Chopping in our analogy.
- Stats = Probability/inference, the heart of machine learning. Recipes/cookbook.
- Calculus = Learning. Moving our error dot to the bottom of the valley. Baking, the actual "cook" step.</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>7. Logistic Regression</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-7.mp3</link>
<pubDate>Sun, 19 Feb 2017 00:00:00 EST</pubDate>
<description>Your first classifier: Logistic Regression. That plus Linear Regression, and you're a 101 supervised learner!

## Resources
You've started Ng's Coursera course (https://www.coursera.org/learn/machine-learning), right? Riight?

## Episode
See Andrew Ng Week 3 Lecture Notes (https://www.coursera.org/learn/machine-learning/resources/Zi29t)</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-7.mp3" length="30495267" type="audio/mpeg"/>
<guid>36b6133d-3018-4be0-a36c-61904aa80a1a</guid>
<itunes:duration>34:19</itunes:duration>
<itunes:subtitle>Your first classifier: Logistic Regression. That plus Linear Regression, and you're a 101 supervised learner!</itunes:subtitle>
<itunes:summary>Your first classifier: Logistic Regression. That plus Linear Regression, and you're a 101 supervised learner!

## Resources
You've started Ng's Coursera course (https://www.coursera.org/learn/machine-learning), right? Riight?

## Episode
See Andrew Ng Week 3 Lecture Notes (https://www.coursera.org/learn/machine-learning/resources/Zi29t)</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>6. Certificates &amp; Degrees</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-6.mp3</link>
<pubDate>Fri, 17 Feb 2017 00:00:00 EST</pubDate>
<description>Discussion on certificates and degrees from Udacity to a Masters degree.

## Resources
- Discussions: 1 (http://canyon289.github.io/DSGuide.html#DSGuide) 2 (https://news.ycombinator.com/item?id=13654127) 3 (http://cole-maclean.github.io/blog/Self%20Taught%20AI/) 4 (https://news.ycombinator.com/item?id=12516441)

## Episode
Self-edify
- Coursera Specialization - flat $500
- Udacity Nanodegree - $200/m (discount if timely completion)
** Great for self-teaching, not recognized degree
** Machine Learning (https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009)
** Self Driving Car (https://www.udacity.com/drive)
** Artificial Intelligence (https://www.udacity.com/ai)

OMSCS (https://www.omscs.gatech.edu/): Great &amp; cheap online masters degree

Portfolio: Most important for getting a job</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-6.mp3" length="14888861" type="audio/mpeg"/>
<guid>a8bd671f-100f-42ff-a68a-cff7763298f6</guid>
<itunes:duration>15:36</itunes:duration>
<itunes:subtitle>Discussion on certificates and degrees from Udacity to a Masters degree.</itunes:subtitle>
<itunes:summary>Discussion on certificates and degrees from Udacity to a Masters degree.

## Resources
- Discussions: 1 (http://canyon289.github.io/DSGuide.html#DSGuide) 2 (https://news.ycombinator.com/item?id=13654127) 3 (http://cole-maclean.github.io/blog/Self%20Taught%20AI/) 4 (https://news.ycombinator.com/item?id=12516441)

## Episode
Self-edify
- Coursera Specialization - flat $500
- Udacity Nanodegree - $200/m (discount if timely completion)
** Great for self-teaching, not recognized degree
** Machine Learning (https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009)
** Self Driving Car (https://www.udacity.com/drive)
** Artificial Intelligence (https://www.udacity.com/ai)

OMSCS (https://www.omscs.gatech.edu/): Great &amp; cheap online masters degree

Portfolio: Most important for getting a job</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>5. Linear Regression</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-5.mp3</link>
<pubDate>Thu, 16 Feb 2017 00:00:00 EST</pubDate>
<description>Introduction to the first machine-learning algorithm, the 'hello world' of supervised learning - Linear Regression

## Resources
- Andrew Ng's Machine Learning Coursera course (https://www.coursera.org/learn/machine-learning) `course:hard` No question, the most essential, important, recommended resource in my entire series _period_. Consider it required, not optional. 

## Episode
See Andrew Ng Week 2 Lecture Notes (https://www.coursera.org/learn/machine-learning/resources/QQx8l)
</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-5.mp3" length="30769356" type="audio/mpeg"/>
<guid>2d2e66dd-d100-4e05-afba-a948de1c956d</guid>
<itunes:duration>33:40</itunes:duration>
<itunes:subtitle>Introduction to the first machine-learning algorithm, the 'hello world' of supervised learning - Linear Regression</itunes:subtitle>
<itunes:summary>Introduction to the first machine-learning algorithm, the 'hello world' of supervised learning - Linear Regression

## Resources
- Andrew Ng's Machine Learning Coursera course (https://www.coursera.org/learn/machine-learning) `course:hard` No question, the most essential, important, recommended resource in my entire series _period_. Consider it required, not optional. 

## Episode
See Andrew Ng Week 2 Lecture Notes (https://www.coursera.org/learn/machine-learning/resources/QQx8l)
</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>4. Algorithms - Intuition</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-4.mp3</link>
<pubDate>Sun, 12 Feb 2017 00:00:00 EST</pubDate>
<description>Overview of machine learning algorithms. Infer/predict -&gt; error/loss -&gt; train/learn. Supervised, unsupervised, reinforcement learning.

## Resources
- Tour of Machine Learning Algorithms (http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms) `article:easy` 
- The Master Algorithm (http://amzn.to/2kLOQjW) `audio:medium` Semi-technical overview of ML basics &amp; main algorithms 

## Episode
Learning (ML)
- 3-step process
** Infer / Predict
** Error / Loss
** Train / Learn
- First as batch from spreadsheet, then "online" going forward
** Pre-train your "model"
** "Examples"
** "Weights"
- Housing cost example
** "Features"
** Infer cost based on num_rooms, sq_foot, etc
** Error / Loss function 

Categories
- Supervised learning
** Vision (CNN)
** Speech (RNN)
- Unsupervised
** Market segmentation
- Reinforcement &amp; Semi-Supervised
** Planning (DQN): Games (chess, Mario); Robot movement</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-4.mp3" length="20773676" type="audio/mpeg"/>
<guid>a7d9b86e-d3aa-4384-a854-792bfcf36e24</guid>
<itunes:duration>21:54</itunes:duration>
<itunes:subtitle>Overview of machine learning algorithms. Infer/predict -&gt; error/loss -&gt; train/learn. Supervised, unsupervised, reinforcement learning.</itunes:subtitle>
<itunes:summary>Overview of machine learning algorithms. Infer/predict -&gt; error/loss -&gt; train/learn. Supervised, unsupervised, reinforcement learning.

## Resources
- Tour of Machine Learning Algorithms (http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms) `article:easy` 
- The Master Algorithm (http://amzn.to/2kLOQjW) `audio:medium` Semi-technical overview of ML basics &amp; main algorithms 

## Episode
Learning (ML)
- 3-step process
** Infer / Predict
** Error / Loss
** Train / Learn
- First as batch from spreadsheet, then "online" going forward
** Pre-train your "model"
** "Examples"
** "Weights"
- Housing cost example
** "Features"
** Infer cost based on num_rooms, sq_foot, etc
** Error / Loss function 

Categories
- Supervised learning
** Vision (CNN)
** Speech (RNN)
- Unsupervised
** Market segmentation
- Reinforcement &amp; Semi-Supervised
** Planning (DQN): Games (chess, Mario); Robot movement</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>3. Inspiration</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-3.mp3</link>
<pubDate>Fri, 10 Feb 2017 00:00:00 EST</pubDate>
<description>Why should you care about AI? Inspirational topics about economic revolution, the singularity, consciousness, and fear.

## Resources
- The Singularity Is Near (http://amzn.to/2lzCqKk) `audio:easy`
- Philosophy of Mind: Brains, Consciousness, and Thinking Machines (Audible (http://amzn.to/2kQGgk5), TGC (https://goo.gl/fDteyi)) `audio:easy`
- Superintelligence (http://amzn.to/2lzLcrL) `audio:easy` doom-and-gloom favorite of Musk, Gates, Hawking.

## Episode
Economics / Automation
- Mental automation (Tax prep; x-rays, surgeons; cars; law; programmers, designers, logos; music, art)
- Is your job safe? (http://www.bbc.com/news/technology-34066941)
- Universal basic income

Singularity (AGI; Singularity; Next stage of evolution)

Consciousness (Functionalism / Computational Theory of Mind / Simulations)

The Scare
- Superintelligence by Nick Bostrom
- Bill Gates, Stephen Hawking, Elon Musk ^</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-3.mp3" length="16382362" type="audio/mpeg"/>
<guid>a0b24583-e253-492c-addc-ee0c0aeb1765</guid>
<itunes:duration>17:41</itunes:duration>
<itunes:subtitle>Why should you care about AI? Inspirational topics about economic revolution, the singularity, consciousness, and fear.</itunes:subtitle>
<itunes:summary>Why should you care about AI? Inspirational topics about economic revolution, the singularity, consciousness, and fear.

## Resources
- The Singularity Is Near (http://amzn.to/2lzCqKk) `audio:easy`
- Philosophy of Mind: Brains, Consciousness, and Thinking Machines (Audible (http://amzn.to/2kQGgk5), TGC (https://goo.gl/fDteyi)) `audio:easy`
- Superintelligence (http://amzn.to/2lzLcrL) `audio:easy` doom-and-gloom favorite of Musk, Gates, Hawking.

## Episode
Economics / Automation
- Mental automation (Tax prep; x-rays, surgeons; cars; law; programmers, designers, logos; music, art)
- Is your job safe? (http://www.bbc.com/news/technology-34066941)
- Universal basic income

Singularity (AGI; Singularity; Next stage of evolution)

Consciousness (Functionalism / Computational Theory of Mind / Simulations)

The Scare
- Superintelligence by Nick Bostrom
- Bill Gates, Stephen Hawking, Elon Musk ^</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>2. What is AI / ML</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-2-fixed.mp3</link>
<pubDate>Thu, 09 Feb 2017 00:00:00 EST</pubDate>
<description>What is artificial intelligence and machine learning? What's the difference? How about compared to statistics and data science? AI history.

## Resources
- Wikipedia:AI (https://en.wikipedia.org/wiki/Artificial_intelligence) `article:easy`
- The Quest for Artificial Intelligence (http://amzn.to/2kRd4Ie) (Free PDF? (http://ai.stanford.edu/~nilsson/QAI/qai.pdf)) `book:hard` AI history
- Machines of Loving Grace (http://amzn.to/2kRcBWq) `audio:easy` AI history

## Episode
What is AI?
- Simulate any intellectual task
- Goals
** Search / planning (eg chess)
** Reasoning / knowledge representation (eg Watson on Jeopardy)
** Perception
** Ability to move and manipulate objects
** Natural language processing (communication)
** Learning
- Applications
** Autonomous vehicles (drones, self-driving cars)
** Medical diagnosis
** Creating art (such as poetry)
** Proving mathematical theorems
** Playing games (such as Chess or Go)
** Search engines
** Online assistants (such as Siri)
** Image recognition in photographs
** Spam filtering
** Prediction of judicial decisions
** Targeting online advertisements 
- When a technique -&gt; mainstream, no longer AI: "AI effect"
** Pre-programming
** Weak AI vs Strong / AGI

What is ML?
- Pattern / Predict / Learn
- Versus AI
** The "whole" (robotics, planning, etc)
** Professional: ML more interesting, subsuming other fields; ML is starter
** Conversation
    "AI when wow-ing or colloquial, ML when being professional. Like "coding" vs "software engineering""
- Versus Stats
- Versus DataScience: professionally; ansense vs analytics

History
- Greek mythology, Golums
- First attempt: Ramon Lull, 13th century
- Davinci's walking animals
- Descartes, Leibniz
- 1700s-1800s: Statistics &amp; Mathematical decision making
** Thomas Bayes: reasoning about the probability of events
** George Boole: logical reasoning / binary algebra
** Gottlob Frege: Propositional logic 
- 1832: Charles Babbage &amp; Ada Byron / Lovelace: designed Analytical Engine (1832), programmable mechanical calculating machines
- 1936: Universal Turing Machine
** Computing Machinery and Intelligence - explored AI!
- 1946: John von Neumann Universal Computing Machine
- 1943: Warren McCulloch &amp; Walter Pitts: cogsci rep of neuron; Frank Rosemblatt uses to create Perceptron (-&gt; neural networks by way of MLP)
- 50s-70s: "AI" coined @Dartmouth workshop 1956 - goal to simulate all aspects of intelligence. John McCarthy, Marvin Minksy, Arthur Samuel, Oliver Selfridge, Ray Solomonoff, Allen Newell, Herbert Simon
** Newell &amp; Simon: Hueristics -&gt; Logic Theories, General Problem Solver
** Slefridge: Computer Vision
** NLP
** Stanford Research Institute: Shakey
** Feigenbaum: Expert systems
** GOFAI / symbolism: operations research / management science; logic-based; knowledge-based / expert systems
- 70s: Lighthill report (James Lighthill), big promises -&gt; AI Winter
- 90s: Data, Computation, Practical Application -&gt; AI back (90s)
** Connectionism optimizations: Geoffrey Hinton: 2006, optimized back propagation
- Bloomberg, 2015 was whopper for AI in industry
- AlphaGo &amp; DeepMind
</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-2-fixed.mp3" length="29646598" type="audio/mpeg"/>
<guid>129d0157-fbda-4cc6-aaae-1c96745c12c9</guid>
<itunes:duration>32:05</itunes:duration>
<itunes:subtitle>What is artificial intelligence and machine learning? What's the difference? How about compared to statistics and data science? AI history.</itunes:subtitle>
<itunes:summary>What is artificial intelligence and machine learning? What's the difference? How about compared to statistics and data science? AI history.

## Resources
- Wikipedia:AI (https://en.wikipedia.org/wiki/Artificial_intelligence) `article:easy`
- The Quest for Artificial Intelligence (http://amzn.to/2kRd4Ie) (Free PDF? (http://ai.stanford.edu/~nilsson/QAI/qai.pdf)) `book:hard` AI history
- Machines of Loving Grace (http://amzn.to/2kRcBWq) `audio:easy` AI history

## Episode
What is AI?
- Simulate any intellectual task
- Goals
** Search / planning (eg chess)
** Reasoning / knowledge representation (eg Watson on Jeopardy)
** Perception
** Ability to move and manipulate objects
** Natural language processing (communication)
** Learning
- Applications
** Autonomous vehicles (drones, self-driving cars)
** Medical diagnosis
** Creating art (such as poetry)
** Proving mathematical theorems
** Playing games (such as Chess or Go)
** Search engines
** Online assistants (such as Siri)
** Image recognition in photographs
** Spam filtering
** Prediction of judicial decisions
** Targeting online advertisements 
- When a technique -&gt; mainstream, no longer AI: "AI effect"
** Pre-programming
** Weak AI vs Strong / AGI

What is ML?
- Pattern / Predict / Learn
- Versus AI
** The "whole" (robotics, planning, etc)
** Professional: ML more interesting, subsuming other fields; ML is starter
** Conversation
    "AI when wow-ing or colloquial, ML when being professional. Like "coding" vs "software engineering""
- Versus Stats
- Versus DataScience: professionally; ansense vs analytics

History
- Greek mythology, Golums
- First attempt: Ramon Lull, 13th century
- Davinci's walking animals
- Descartes, Leibniz
- 1700s-1800s: Statistics &amp; Mathematical decision making
** Thomas Bayes: reasoning about the probability of events
** George Boole: logical reasoning / binary algebra
** Gottlob Frege: Propositional logic 
- 1832: Charles Babbage &amp; Ada Byron / Lovelace: designed Analytical Engine (1832), programmable mechanical calculating machines
- 1936: Universal Turing Machine
** Computing Machinery and Intelligence - explored AI!
- 1946: John von Neumann Universal Computing Machine
- 1943: Warren McCulloch &amp; Walter Pitts: cogsci rep of neuron; Frank Rosemblatt uses to create Perceptron (-&gt; neural networks by way of MLP)
- 50s-70s: "AI" coined @Dartmouth workshop 1956 - goal to simulate all aspects of intelligence. John McCarthy, Marvin Minksy, Arthur Samuel, Oliver Selfridge, Ray Solomonoff, Allen Newell, Herbert Simon
** Newell &amp; Simon: Hueristics -&gt; Logic Theories, General Problem Solver
** Slefridge: Computer Vision
** NLP
** Stanford Research Institute: Shakey
** Feigenbaum: Expert systems
** GOFAI / symbolism: operations research / management science; logic-based; knowledge-based / expert systems
- 70s: Lighthill report (James Lighthill), big promises -&gt; AI Winter
- 90s: Data, Computation, Practical Application -&gt; AI back (90s)
** Connectionism optimizations: Geoffrey Hinton: 2006, optimized back propagation
- Bloomberg, 2015 was whopper for AI in industry
- AlphaGo &amp; DeepMind
</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
<item>
<title>1. Introduction</title>
<link>http://ocdevel.com/files/podcasts/machine-learning/ml-1.mp3</link>
<pubDate>Wed, 01 Feb 2017 00:00:00 EST</pubDate>
<description>Introduction to the Machine Learning Guide

Who am I: Tyler Renelle (https://www.linkedin.com/in/lefnire)

What is this podcast? 
- "Middle" level overview (deeper than a bird's eye view of machine learning; higher than math equations)
- No math/programming experience required

Who is it for
- Anyone curious about machine learning fundamentals
- Aspiring machine learning developers (maybe transitioning from web/mobile development)

Why audio?
- Supplementary content for commute/exercise/chores will help solidify your book/course-work

What it's not
- News and Interviews
** TWiML and AI (https://twimlai.com)
** O'Reilly Data Show (https://www.oreilly.com/topics/oreilly-data-show-podcast)
** Talking machines (http://www.thetalkingmachines.com/)
- Misc Topics
** Linear Digressions (http://lineardigressions.com/)
** Data Skeptic (https://dataskeptic.com/)
** Partially Derivative (http://partiallyderivative.com/)
- iTunesU issues
- Learning machines 101 (http://www.learningmachines101.com/)

Planned episodes
- What is AI/ML: definition, comparison, history
- Inspiration: automation, singularity, consciousness
- ML Intuition: learning basics (infer/error/train); supervised/unsupervised/reinforcement; applications
- Math overview: linear algebra, statistics, calculus
- Linear models: supervised (regression, classification); unsupervised
- Parts: regularization, performance evaluation, dimensionality reduction, etc
- Deep models: neural networks, recurrent neural networks (RNNs), convolutional neural networks (convnets/CNNs)
- Languages and Frameworks: Python vs R vs Java vs C/C++ vs MATLAB, etc; TensorFlow vs Torch vs Theano vs Spark, etc
</description>
<enclosure url="http://ocdevel.com/files/podcasts/machine-learning/ml-1.mp3" length="11886227" type="audio/mpeg"/>
<guid>a9bf6e09-aa7e-4126-9e36-22b152419c8f</guid>
<itunes:duration>12:34</itunes:duration>
<itunes:subtitle>Introduction to the Machine Learning Guide</itunes:subtitle>
<itunes:summary>Introduction to the Machine Learning Guide

Who am I: Tyler Renelle (https://www.linkedin.com/in/lefnire)

What is this podcast? 
- "Middle" level overview (deeper than a bird's eye view of machine learning; higher than math equations)
- No math/programming experience required

Who is it for
- Anyone curious about machine learning fundamentals
- Aspiring machine learning developers (maybe transitioning from web/mobile development)

Why audio?
- Supplementary content for commute/exercise/chores will help solidify your book/course-work

What it's not
- News and Interviews
** TWiML and AI (https://twimlai.com)
** O'Reilly Data Show (https://www.oreilly.com/topics/oreilly-data-show-podcast)
** Talking machines (http://www.thetalkingmachines.com/)
- Misc Topics
** Linear Digressions (http://lineardigressions.com/)
** Data Skeptic (https://dataskeptic.com/)
** Partially Derivative (http://partiallyderivative.com/)
- iTunesU issues
- Learning machines 101 (http://www.learningmachines101.com/)

Planned episodes
- What is AI/ML: definition, comparison, history
- Inspiration: automation, singularity, consciousness
- ML Intuition: learning basics (infer/error/train); supervised/unsupervised/reinforcement; applications
- Math overview: linear algebra, statistics, calculus
- Linear models: supervised (regression, classification); unsupervised
- Parts: regularization, performance evaluation, dimensionality reduction, etc
- Deep models: neural networks, recurrent neural networks (RNNs), convolutional neural networks (convnets/CNNs)
- Languages and Frameworks: Python vs R vs Java vs C/C++ vs MATLAB, etc; TensorFlow vs Torch vs Theano vs Spark, etc
</itunes:summary>
<itunes:image href="http://ocdevel.com/files/podcasts/machine-learning/art.jpg"/>
<itunes:keywords>machine,learning,ml,introduction,artificial,intelligence,ai</itunes:keywords>
<itunes:explicit>no</itunes:explicit>
        </item>
    </channel>
</rss>